apiVersion: workloads.x-k8s.io/v1alpha1
kind: RoleBasedGroup
metadata:
  name: vllm-distributed
spec:
  roles:
    - name: worker
      replicas: 1
      workload:
        apiVersion: leaderworkerset.x-k8s.io/v1
        kind: LeaderWorkerSet
      leaderWorkerSet:
        size: 2
        patchLeaderTemplate:
          metadata:
            labels:
              role: leader
          spec:
            containers:
              - name: vllm
                command:
                  - sh
                  - -c
                  - "bash /vllm-workspace/examples/online_serving/multi-node-serving.sh leader --ray_cluster_size=$(LWS_GROUP_SIZE); 
                  python3 -m vllm.entrypoints.openai.api_server --port 8000 --model /models/Qwen2.5-7B-Instruct/ --trust-remote-code --gpu-memory-utilization 0.85 --tensor-parallel-size=2"
            readinessProbe:
              tcpSocket:
                port: 8000
              initialDelaySeconds: 15
              periodSeconds: 10
        patchWorkerTemplate:
          spec:
            containers:
              - name: vllm
                command:
                  - sh
                  - -c
                  - "bash /vllm-workspace/examples/online_serving/multi-node-serving.sh worker --ray_address=$(LWS_LEADER_ADDRESS)"
      template:
        metadata:
          labels:
            inference-framework: vllm
            inference-stack.io/monitoring: "enabled"
        spec:
          volumes:
            - name: model
              persistentVolumeClaim:
                claimName: llm-model
            - name: shm
              emptyDir:
                medium: Memory
                sizeLimit: 15Gi
          containers:
            - name: vllm
              image: kube-ai-registry.cn-shanghai.cr.aliyuncs.com/kube-ai/vllm:v0.8.4
              env:
                - name: VLLM_USE_V1
                  value: "0"
              resources:
                limits:
                  nvidia.com/gpu: "1"
                requests:
                  nvidia.com/gpu: "1"
              volumeMounts:
                - name: model
                  mountPath: /models/Qwen2.5-7B-Instruct
                - name: shm
                  mountPath: /dev/shm
              ports:
                - name: http
                  containerPort: 8000
---
apiVersion: v1
kind: Service
metadata:
  name: vllm-service
spec:
  type: ClusterIP
  ports:
    - port: 8000
      protocol: TCP
      targetPort: 8000
  selector:
    rolebasedgroup.workloads.x-k8s.io/name: vllm-distributed
    role: leader