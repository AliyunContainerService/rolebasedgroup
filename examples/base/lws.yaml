apiVersion: v1
kind: PodTemplate
metadata:
  name: vllm-template
template:
  metadata:
    labels:
      inference-framework: vllm
      inference-stack.io/monitoring: "enabled"
  spec:
    volumes:
      - name: model
        persistentVolumeClaim:
          claimName: llm-model
      - name: shm
        emptyDir:
          medium: Memory
          sizeLimit: 15Gi
    containers:
      - name: vllm
        image: kube-ai-registry.cn-shanghai.cr.aliyuncs.com/kube-ai/vllm:v0.8.4
        command:
          - sh
          - -c
          - VLLM_USE_V1=0 vllm serve /models/Qwen2.5-7B-Instruct/ --trust-remote-code --port=8000 --max-model-len 2048 --gpu-memory-utilization 0.95 --enforce-eager
        resources:
          limits:
            nvidia.com/gpu: "1"
          requests:
            nvidia.com/gpu: "1"
        volumeMounts:
          - name: model
            mountPath: /mount/model
          - name: shm
            mountPath: /dev/shm
        ports:
          - name: http
            containerPort: 8000
---
apiVersion: workloads.x-k8s.io/v1alpha1
kind: RoleBasedGroup
metadata:
  name: vllm-distributed
spec:
  roles:
    - name: worker
      replicas: 4
      workload:
        apiVersion: leaderworkerset.x-k8s.io/v1
        kind: LeaderWorkerSet
      leaderWorkerSet:
        size: 2
        patches:
          leaderTemplate:
            metadata:
              labels:
                role: leader
            spec:
              containers:
                - name: vllm
                  command:
                    - "/vllm-workspace/ray_init.sh leader --ray_cluster_size=$(RBG_GROUP_SIZE); 
                    - vllm serve /models/Qwen2.5-Coder-1.5B-Instruct --trust-remote-code \
                    --port=8000 --max-model-len 2048 --gpu-memory-utilization 0.95 --enforce-eager \
                    --dtype=half --tensor-parallel-size=2"
          workerTemplate:
            spec:
              containers:
                - name: vllm
                  command:
                    - "/vllm-workspace/ray_init.sh worker --ray_address=vllm-multi-nodes-leader"
      podTemplateRef: vllm-template
