apiVersion: workloads.x-k8s.io/v1alpha1
kind: RoleBasedGroup
metadata:
  name: vllm-multi-nodes
spec:
  roles:
    - name: leader
      replicas: 1
      template:
        spec:
          volumes:
            - name: model
              persistentVolumeClaim:
                claimName: llm-model
            - name: dshm
              emptyDir:
                medium: Memory
                sizeLimit: 1Gi
          containers:
            - name: vllm
              image: kube-ai-registry.cn-shanghai.cr.aliyuncs.com/kube-ai/vllm:v0.8.4
              command:
                - sh
                - -c
                - "/vllm-workspace/ray_init.sh leader --ray_cluster_size=$(RBG_GROUP_SIZE); 
                   vllm serve /models/Qwen2.5-Coder-1.5B-Instruct --trust-remote-code \
                  --port=8000 --max-model-len 2048 --gpu-memory-utilization 0.95 --enforce-eager \
                  --dtype=half --tensor-parallel-size=2"
              ports:
                - containerPort: 8000
              resources:
                limits:
                  nvidia.com/gpu: "1"
              volumeMounts:
                - mountPath: /models/Qwen2.5-Coder-1.5B-Instruct
                  name: model
                - mountPath: /dev/shm
                  name: dshm
      servicePorts:
        - name: http
          port: 8000
          targetPort: 8000

    - name: worker
      replicas: 1
      dependencies: ["leader"]
      template:
        spec:
          volumes:
            - name: model
              persistentVolumeClaim:
                claimName: llm-model
            - name: dshm
              emptyDir:
                medium: Memory
                sizeLimit: 1Gi
          containers:
            - name: vllm
              image: kube-ai-registry.cn-shanghai.cr.aliyuncs.com/kube-ai/vllm:v0.8.4
              command:
                - sh
                - -c
                - /vllm-workspace/ray_init.sh worker --ray_address=vllm-multi-nodes-leader
              ports:
                - containerPort: 8000
              resources:
                limits:
                  nvidia.com/gpu: "1"
              volumeMounts:
                - mountPath: /models/Qwen2.5-Coder-1.5B-Instruct
                  name: model
                - mountPath: /dev/shm
                  name: dshm
              
    
