apiVersion: workloads.x-k8s.io/v1alpha1
kind: RoleBasedGroup
metadata:
  name: vllm
spec:
  roles:
    - name: worker
      replicas: 1
      workload:
        apiVersion: apps/v1
        kind: Deployment
      template:
        spec:
          volumes:
            - name: model
              persistentVolumeClaim:
                claimName: llm-model
          containers:
            - name: vllm
              image: kube-ai-registry.cn-shanghai.cr.aliyuncs.com/kube-ai/vllm:v0.8.4
              command:
                - sh
                - -c
                - "vllm serve /models/Qwen2.5-Coder-1.5B-Instruct --trust-remote-code --port=8000 --max-model-len 2048 --gpu-memory-utilization 0.95 --enforce-eager --dtype=half"
              ports:
                - containerPort: 8000
              resources:
                limits:
                  nvidia.com/gpu: "1"
              volumeMounts:
                - mountPath: /models/Qwen2.5-Coder-1.5B-Instruct
                  name: model