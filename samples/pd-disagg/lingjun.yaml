# for ACK + LingJun Node
---
apiVersion: workloads.x-k8s.io/v1alpha1
kind: RoleBasedGroup
metadata:
  name: lingjun-pd
spec:
  roles:
    - name: scheduler
      replicas: 1
      template:
        metadata:
          labels:
            # for acs
            inference-stack.io/monitoring: "enabled"
            alibabacloud.com/compute-class: performance
        spec:
          volumes:
            - name: model
              persistentVolumeClaim:
                claimName: llm-model
            - name: scheduler-run
              configMap:
                name: scheduler-run
                defaultMode: 0777
          containers:
            - name: scheduler
              image: egslingjun-registry.cn-wulanchabu.cr.aliyuncs.com/egslingjun/training-nv-pytorch:25.04-dpd-250423
              command:
                - sh
                - -c
                - /app/run/scheduler-run.sh --instances-config-path=/etc/patio/instance-config.yaml --tokenizer-path=/models/Qwen2.5-7B-Instruct/tokenizer.json --port=8008  --PD-mode=advanced --scheduler-mode=LoadBalance --model-path=/models/Qwen2.5-7B-Instruct
              ports:
                - containerPort: 8008
                  name: http
              resources:
                requests:
                  cpu: "4"
                  memory: 8Gi
              volumeMounts:
                - mountPath: /models/Qwen2.5-7B-Instruct/
                  name: model
                - mountPath: /app/run/
                  name: scheduler-run
                - mountPath: /etc/patio/
                  name: patio-group-config
          tolerations:
            - key: "node-role.alibabacloud.com/lingjun"
              operator: "Exists"
      engineRuntimes:
        - profileName: patio-runtime
          containers:
            - name: patio-runtime
              env:
                - name: TOPO_CONFIG_FILE
                  value: /etc/patio/instance-config.yaml

    - name: prefill
      replicas: 1
      template:
        metadata:
          labels:
            inference-engine: vllm
            inference-stack.io/monitoring: "enabled"
            # for acs
            alibabacloud.com/compute-class: gpu
            alibabacloud.com/gpu-model-series: L20
        spec:
          volumes:
            - name: model
              persistentVolumeClaim:
                claimName: llm-model
          containers:
            - name: vllm
              image: egslingjun-registry.cn-wulanchabu.cr.aliyuncs.com/egslingjun/training-nv-pytorch:25.04-dpd-250423
              command:
                - sh
                - -c
                - VLLM_USE_V1=0 vllm serve /models/Qwen2.5-7B-Instruct/ --trust-remote-code --port=8000 --max-model-len 2048  --gpu-memory-utilization 0.95  --enable-chunked-prefill=False  --no-enable-prefix-caching
              ports:
                - containerPort: 8000
                  name: http
              resources:
                limits:
                  nvidia.com/gpu: "1"
                requests:
                  nvidia.com/gpu: "1"
              volumeMounts:
                - mountPath: /models/Qwen2.5-7B-Instruct/
                  name: model
              env:
                - name: NCCL_SOCKET_IFNAME
                  value: eth0
          tolerations:
            - key: "node-role.alibabacloud.com/lingjun"
              operator: "Exists"
      engineRuntimes:
        - profileName: patio-runtime
          containers:
            - name: patio-runtime
              args:
                - --instance-info={"topo_type":"LingJun","data":{"work_role":"prefill-only"}}
              env:
                - name: TOPO_COLLECTOR_ENDPOINT
                  value: http://lingjun-pd-scheduler-0.lingjun-pd-scheduler:9091

    - name: decode
      replicas: 1
      template:
        metadata:
          labels:
            inference-framework: vllm
            # for acs
            inference-stack.io/monitoring: "enabled"
            alibabacloud.com/compute-class: gpu
            alibabacloud.com/gpu-model-series: H20
        spec:
          volumes:
            - name: model
              persistentVolumeClaim:
                claimName: llm-model
          containers:
            - name: vllm
              image: egslingjun-registry.cn-wulanchabu.cr.aliyuncs.com/egslingjun/training-nv-pytorch:25.04-dpd-250423
              command:
                - sh
                - -c
                - VLLM_USE_V1=0 vllm serve /models/Qwen2.5-7B-Instruct/ --trust-remote-code --port=8000 --max-model-len 2048  --gpu-memory-utilization 0.95  --enable-chunked-prefill=False  --no-enable-prefix-caching
              ports:
                - containerPort: 8000
                  name: http
              resources:
                limits:
                  nvidia.com/gpu: "1"
                requests:
                  nvidia.com/gpu: "1"
              volumeMounts:
                - mountPath: /models/Qwen2.5-7B-Instruct/
                  name: model
              env:
                - name: NCCL_SOCKET_IFNAME
                  value: eth0
          tolerations:
            - key: "node-role.alibabacloud.com/lingjun"
              operator: "Exists"
      engineRuntimes:
        - profileName: patio-runtime
          containers:
            - name: patio-runtime
              args:
                - --instance-info={"topo_type":"LingJun","data":{"work_role":"decode-only"}}
              env:
                - name: TOPO_COLLECTOR_ENDPOINT
                  value: http://lingjun-pd-scheduler-0.lingjun-pd-scheduler:9091

