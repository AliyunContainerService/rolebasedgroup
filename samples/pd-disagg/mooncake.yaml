apiVersion: workloads.x-k8s.io/v1alpha1
kind: RoleBasedGroup
metadata:
  name: mooncake-pd
spec:
  roles:
    - name: scheduler
      replicas: 1
      template:
        spec:
          volumes:
            - name: model
              persistentVolumeClaim:
                claimName: llm-model
          containers:
            - name: scheduler
              image: registry-vpc.ap-southeast-1.aliyuncs.com/zibai-test/mooncake:v2
              imagePullPolicy: Always
              command:
                - sh
                - -c
                - mooncake_master --port 50001
              volumeMounts:
                - mountPath: pre
                  name: model

    - name: prefill
      replicas: 1
      template:
        spec:
          volumes:
            - name: model
              persistentVolumeClaim:
                claimName: llm-model
          containers:
            - name: vllm-prefill
              image: registry-vpc.ap-southeast-1.aliyuncs.com/zibai-test/mooncake:v2
              imagePullPolicy: Always
              command:
                - sh
                - -c
                - >-
                  VLLM_LOGGING_LEVEL=debug MOONCAKE_CONFIG_PATH=/etc/patio/mooncake.json VLLM_USE_V1=0 python3 -m vllm.entrypoints.openai.api_server
                  --model /models/Qwen2.5-7B-Instruct/
                  --port 8000
                  --max-model-len 2048
                  --gpu-memory-utilization 0.95
                  --kv-transfer-config '{"kv_connector":"MooncakeStoreConnector","kv_role":"kv_producer"}'
              ports:
                - containerPort: 8000
              resources:
                limits:
                  nvidia.com/gpu: "1"
              volumeMounts:
                - mountPath: /models/Qwen2.5-7B-Instruct/
                  name: model
      runtimeEngine:
        args:
          - >-
            --instance-info={
              "topo_type": "Mooncake",
              "data": {
                "metadata_server": "etcd://etcd:2379",
                "protocol": "tcp",
                "device_name": "",
                "master_server_address": "$(GROUP_NAME)-scheduler-0.$(GROUP_NAME)-scheduler:50001"
              }
            }
        env:
          - name: TOPO_CONFIG_FILE
            value: /etc/patio/mooncake.json
        mountGroupConfig: true
        groupConfigMountPath: /etc/patio

    - name: decode
      replicas: 1
      template:
        spec:
          volumes:
            - name: model
              persistentVolumeClaim:
                claimName: llm-model
          containers:
            - name: vllm-decode
              image: registry-vpc.ap-southeast-1.aliyuncs.com/zibai-test/mooncake:v2
              imagePullPolicy: Always
              command:
                - sh
                - -c
                - >-
                  VLLM_LOGGING_LEVEL=debug MOONCAKE_CONFIG_PATH=/etc/patio/mooncake.json VLLM_USE_V1=0 python3 -m vllm.entrypoints.openai.api_server
                  --model /models/Qwen2.5-7B-Instruct/
                  --port 8000
                  --max-model-len 2048
                  --gpu-memory-utilization 0.95
                  --kv-transfer-config '{"kv_connector":"MooncakeStoreConnector","kv_role":"kv_consumer"}'
              ports:
                - containerPort: 8000
              resources:
                limits:
                  nvidia.com/gpu: "1"
              volumeMounts:
                - mountPath: /models/Qwen2.5-7B-Instruct/
                  name: model
      runtimeEngine:
        args:
          - >-
            --instance-info={
              "topo_type": "Mooncake",
              "data": {
                "metadata_server": "etcd://etcd:2379",
                "protocol": "tcp",
                "device_name": "",
                "master_server_address": "$(GROUP_NAME)-scheduler-0.$(GROUP_NAME)-scheduler:50001"
              }
            }
        env:
          - name: TOPO_CONFIG_FILE
            value: /etc/patio/mooncake.json
        mountGroupConfig: true
        groupConfigMountPath: /etc/patio
