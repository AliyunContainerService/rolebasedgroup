apiVersion: workloads.x-k8s.io/v1alpha1
kind: RoleBasedGroup
metadata:
  name: runtime-metric-example
spec:
  roles:
    - name: vllm
      replicas: 1
      template:
        spec:
          volumes:
            - name: model
              persistentVolumeClaim:
                claimName: llm-model
          containers:
            - name: vllm
              image: kube-ai-registry.cn-shanghai.cr.aliyuncs.com/kube-ai/vllm:v0.8.4
              command:
                - sh
                - -c
                - vllm serve /models/Qwen2.5-7B-Instruct/ --trust-remote-code --port=8000 --max-model-len 2048 --gpu-memory-utilization 0.95 --enforce-eager
              ports:
                - containerPort: 8000
              resources:
                limits:
                  nvidia.com/gpu: "1"
              volumeMounts:
                - mountPath: /models/Qwen2.5-7B-Instruct/
                  name: model
      engineRuntimes:
        - profileName: patio-runtime
          containers:
            - name: patio-runtime
              env:
                - name: INFERENCE_ENGINE
                  value: vllm
                - name: INFERENCE_ENGINE_ENDPOINT
                  value: http://localhost:8000